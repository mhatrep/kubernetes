How to implement centralized logging using an ElasticsSearch, Fluent Bit, and Kibana (EFK) stack on Kubernetes.

# Reference - https://www.katacoda.com/courses/kubernetes/kubernetes-observability-efk-by-javajon

master $ kubectl version --short && kubectl cluster-info && kubectl get nodes
Client Version: v1.14.0
Server Version: v1.14.0
Kubernetes master is running at https://172.17.0.24:6443
dash-kubernetes-dashboard is running at https://172.17.0.24:6443/api/v1/namespaces/kube-system/services/dash-kubernetes-dashboard:http/proxy
KubeDNS is running at https://172.17.0.24:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   63m   v1.14.0
node01   Ready    <none>   63m   v1.14.0
master $ helm version --short
Client: v2.13.1+g618447c
Server: v2.13.1+g618447c
master $ export TOKEN=$(kubectl describe secret $(kubectl get secret | awk '/^dashboard-token-/{print $1}') | awk '$1=="token:"{print $2}') &&
> echo -e "\n--- Copy and paste this token for dashboard access --\n$TOKEN\n---"

--- Copy and paste this token for dashboard access --
eyJhbGciOi..........


----------------------------------------------------

master $ mkdir -p /mnt/data/efk-master && kubectl create -f pv-master.yaml
persistentvolume/efk-master-volume createdmaster $ mkdir -p /mnt/data/efk-data && kubectl create -f pv-data.yaml
persistentvolume/efk-data-volume created
master $ helm install stable/elasticsearch --name=elasticsearch --namespace=logs \
> --set client.replicas=1 \
> --set master.replicas=1 \
> --set cluster.env.MINIMUM_MASTER_NODES=1 \
> --set cluster.env.RECOVER_AFTER_MASTER_NODES=1 \
> --set cluster.env.EXPECTED_MASTER_NODES=1 \
> --set data.replicas=1 \
> --set data.heapSize=300m \
> --set master.persistence.storageClass=elasticsearch-master \> --set master.persistence.size=5Gi \
> --set data.persistence.storageClass=elasticsearch-data \
> --set data.persistence.size=5Gi
NAME:   elasticsearch
LAST DEPLOYED: Sat Oct 19 01:31:49 2019NAMESPACE: logsSTATUS: DEPLOYEDRESOURCES:==> v1/ConfigMapNAME                DATA  AGE
elasticsearch       4     0s
elasticsearch-test  1     0s

==> v1/Deployment
NAME                  READY  UP-TO-DATE  AVAILABLE  AGE
elasticsearch-client  0/1    1           0          0s

==> v1/Pod(related)
NAME                                   READY  STATUS    RESTARTS  AGE
elasticsearch-client-5b86bdd9d7-ttn6n  0/1    Init:0/1  0         0s
elasticsearch-data-0                   0/1    Pending   0         0s
elasticsearch-master-0                 0/1    Pending   0         0s

==> v1/Service
NAME                     TYPE       CLUSTER-IP   EXTERNAL-IP  PORT(S)   AGE
elasticsearch-client     ClusterIP  10.97.54.94  <none>       9200/TCP  0s
elasticsearch-discovery  ClusterIP  None         <none>       9300/TCP  0s

==> v1/ServiceAccount
NAME                  SECRETS  AGE
elasticsearch-client  1        0s
elasticsearch-data    1        0s
elasticsearch-master  1        0s

==> v1/StatefulSet
NAME                  READY  AGE
elasticsearch-data    0/1    0s
elasticsearch-master  0/1    0s


NOTES:
The elasticsearch cluster has been installed.

Elasticsearch can be accessed:

  * Within your cluster, at the following DNS name at port 9200:

    elasticsearch-client.logs.svc

  * From outside the cluster, run these commands in the same shell:

    export POD_NAME=$(kubectl get pods --namespace logs -l "app=elasticsearch,component=client,release=elasticsearch" -ojsonpath="{.items[0].metadata.name}")
    echo "Visit http://127.0.0.1:9200 to use Elasticsearch"
    kubectl port-forward --namespace logs $POD_NAME 9200:9200
    

----------------------------------------------------

master $ helm install stable/fluent-bit --name=fluent-bit --namespace=logs --set backend.type=es --set backend.es.host=elasticsearch-client
NAME:   fluent-bit
LAST DEPLOYED: Sat Oct 19 01:32:33 2019
NAMESPACE: logs
STATUS: DEPLOYED

RESOURCES:
==> v1/ClusterRole
NAME        AGE
fluent-bit  1s

==> v1/ClusterRoleBinding
NAME        AGE
fluent-bit  1s

==> v1/ConfigMap
NAME               DATA  AGE
fluent-bit-config  6     1s
fluent-bit-test    2     1s
==> v1/DaemonSet
NAME        DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGE
fluent-bit  1        1        0      1           0          <none>         1s

==> v1/Pod(related)
NAME              READY  STATUS   RESTARTS  AGE
fluent-bit-snp2j  0/1    Pending  0         1s

==> v1/Secret
NAME                      TYPE    DATA  AGE
fluent-bit-es-tls-secret  Opaque  1     1s

==> v1/ServiceAccount
NAME        SECRETS  AGE
fluent-bit  1        1s


NOTES:
fluent-bit is now running.

It will forward all container logs to the svc named elasticsearch-client on port: 9200


----------------------------------------------------

master $ helm install stable/kibana --name=kibana --namespace=logs --set env.ELASTICSEARCH_HOSTS=http://elasticsearch-client:9200 --set service.type=NodePort --set service.nodePort=31000
NAME:   kibanaLAST DEPLOYED: Sat Oct 19 01:33:13 2019
NAMESPACE: logs
STATUS: DEPLOYED

RESOURCES:
==> v1/ConfigMap
NAME         DATA  AGE
kibana       1     0s
kibana-test  1     0s

==> v1/Deployment
NAME    READY  UP-TO-DATE  AVAILABLE  AGE
kibana  0/1    1           0          0s

==> v1/Pod(related)
NAME                     READY  STATUS             RESTARTS  AGE
kibana-65cbb5f4d9-ctdfc  0/1    ContainerCreating  0         0s

==> v1/Service
NAME    TYPE      CLUSTER-IP    EXTERNAL-IP  PORT(S)        AGE
kibana  NodePort  10.99.134.29  <none>       443:31000/TCP  0s


NOTES:
To verify that kibana has started, run:

  kubectl --namespace=logs get pods -l "app=kibana"

Kibana can be accessed:

  * From outside the cluster, run these commands in the same shell:

    export NODE_PORT=$(kubectl get --namespace logs -o jsonpath="{.spec.ports[0].nodePort}" services kibana)
    export NODE_IP=$(kubectl get nodes --namespace logs -o jsonpath="{.items[0].status.addresses[0].address}")
    echo http://$NODE_IP:$NODE_PORT
    
----------------------------------------------------

>>> watch kubectl get deployments,pods,services --namespace=logs


----------------------------------------------------

master $ kubectl run random-logger --image=chentex/random-logger --generator=run-pod/v1
kubectl run random-logger --image=chentex/random-logger --generator=run-pod/v1
pod/random-logger created
master $ kubectl run random-logger --image=chentex/random-logger --generator=run-pod/v1
Error from server (AlreadyExists): pods "random-logger" already exists
master $ kubectl logs pod/random-logger
master $

----------------------------------------------------



